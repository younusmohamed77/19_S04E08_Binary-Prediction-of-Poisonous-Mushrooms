{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903fa0cd-3b60-47ea-aefc-de5fa02b3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8429e98-cde6-4402-a92a-76f89ca20912",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor.load(r\"C:\\Users\\Hi\\My Works\\My Py Scripts\\Git Repos\\19_S04E08_Binary Prediction of Poisonous Mushrooms\\AutogluonModels\\ag-20240820_151425\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc44f9f2-9eb5-494d-be66-9354bae3cfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: WeightedEnsemble_L2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hi\\AppData\\Local\\Temp\\ipykernel_7596\\1233827583.py:1: DeprecationWarning: `get_model_best` has been deprecated and will be removed in version 1.2. Please use `model_best` instead. This will raise an error in the future!\n",
      "  best_model = predictor.get_model_best()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Print details about the best model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Model:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_model)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures used:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_model\u001b[38;5;241m.\u001b[39mfeatures)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'params'"
     ]
    }
   ],
   "source": [
    "# best_model = predictor.get_model_best()\n",
    "\n",
    "# # Print details about the best model\n",
    "# print(\"Best Model:\", best_model)\n",
    "# print(\"Hyperparameters:\", best_model.params)\n",
    "# print(\"Features used:\", best_model.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b7c4e9-7381-4a2f-ae0b-bd59a7fa2b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: WeightedEnsemble_L2\n",
      "Hyperparameters: {'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}\n",
      "Features used: ['LightGBMXT_BAG_L1', 'LightGBM_BAG_L1']\n"
     ]
    }
   ],
   "source": [
    "# Get the name of the best model\n",
    "best_model_name = predictor.model_best\n",
    "\n",
    "# Load the actual best model object\n",
    "best_model = predictor._trainer.load_model(best_model_name)\n",
    "\n",
    "# Print details about the best model\n",
    "print(\"Best Model:\", best_model_name)\n",
    "print(\"Hyperparameters:\", best_model.params)\n",
    "print(\"Features used:\", best_model.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f18efe1-9fd8-40cf-9775-3d6bcbfc2539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model 1: LightGBMXT_BAG_L1\n",
      "Hyperparameters: {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}\n",
      "Features used: ['cap-diameter', 'stem-height', 'stem-width', 'cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color', 'stem-root', 'stem-surface', 'stem-color', 'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color', 'habitat', 'season']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StackerEnsembleModel' object has no attribute 'get_preprocessor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_model_1\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures used:\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_model_1\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m----> 6\u001b[0m preprocessing_1 \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preprocessor\u001b[49m()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing steps for LightGBMXT_BAG_L1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessing_1)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load and inspect the second base model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StackerEnsembleModel' object has no attribute 'get_preprocessor'"
     ]
    }
   ],
   "source": [
    "# # Load and inspect the first base model\n",
    "# base_model_1 = predictor._trainer.load_model('LightGBMXT_BAG_L1')\n",
    "# print(\"Base Model 1: LightGBMXT_BAG_L1\")\n",
    "# print(\"Hyperparameters:\", base_model_1.params)\n",
    "# print(\"Features used:\", base_model_1.features)\n",
    "# preprocessing_1 = base_model_1.get_preprocessor()\n",
    "# print(\"Preprocessing steps for LightGBMXT_BAG_L1:\", preprocessing_1)\n",
    "\n",
    "# # Load and inspect the second base model\n",
    "# base_model_2 = predictor._trainer.load_model('LightGBM_BAG_L1')\n",
    "# print(\"Base Model 2: LightGBM_BAG_L1\")\n",
    "# print(\"Hyperparameters:\", base_model_2.params)\n",
    "# print(\"Features used:\", base_model_2.features)\n",
    "# preprocessing_2 = base_model_2.get_preprocessor()\n",
    "# print(\"Preprocessing steps for LightGBM_BAG_L1:\", preprocessing_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d1a020-e65a-49bb-87c0-d986a0321a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AutoTrainer' object has no attribute 'load_child'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load base models directly\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m base_model_1 \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_child\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBMXT_BAG_L1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m base_model_2 \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mload_child(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM_BAG_L1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print details about the first base model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AutoTrainer' object has no attribute 'load_child'"
     ]
    }
   ],
   "source": [
    "# # Load base models directly\n",
    "# base_model_1 = predictor._trainer.load_child('LightGBMXT_BAG_L1')\n",
    "# base_model_2 = predictor._trainer.load_child('LightGBM_BAG_L1')\n",
    "\n",
    "# # Print details about the first base model\n",
    "# print(\"Base Model 1: LightGBMXT_BAG_L1\")\n",
    "# print(\"Hyperparameters:\", base_model_1.params)\n",
    "# print(\"Features used:\", base_model_1.features)\n",
    "\n",
    "# # Attempt to print preprocessing steps, if available\n",
    "# try:\n",
    "#     preprocessing_1 = base_model_1.get_preprocessor()\n",
    "#     print(\"Preprocessing steps for LightGBMXT_BAG_L1:\", preprocessing_1)\n",
    "# except AttributeError:\n",
    "#     print(\"No separate preprocessor for this model; preprocessing was likely done prior to stacking.\")\n",
    "\n",
    "# # Print details about the second base model\n",
    "# print(\"Base Model 2: LightGBM_BAG_L1\")\n",
    "# print(\"Hyperparameters:\", base_model_2.params)\n",
    "# print(\"Features used:\", base_model_2.features)\n",
    "\n",
    "# # Attempt to print preprocessing steps, if available\n",
    "# try:\n",
    "#     preprocessing_2 = base_model_2.get_preprocessor()\n",
    "#     print(\"Preprocessing steps for LightGBM_BAG_L1:\", preprocessing_2)\n",
    "# except AttributeError:\n",
    "#     print(\"No separate preprocessor for this model; preprocessing was likely done prior to stacking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb361a12-0b70-474e-8a46-22da1a91e879",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AutoTrainer' object has no attribute '_load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load base models directly using _load_model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m base_model_1 \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBMXT_BAG_L1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m base_model_2 \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39m_load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM_BAG_L1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print details about the first base model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AutoTrainer' object has no attribute '_load_model'"
     ]
    }
   ],
   "source": [
    "# Load base models directly using _load_model\n",
    "base_model_1 = predictor._trainer._load_model('LightGBMXT_BAG_L1')\n",
    "base_model_2 = predictor._trainer._load_model('LightGBM_BAG_L1')\n",
    "\n",
    "# Print details about the first base model\n",
    "print(\"Base Model 1: LightGBMXT_BAG_L1\")\n",
    "print(\"Hyperparameters:\", base_model_1.params)\n",
    "print(\"Features used:\", base_model_1.features)\n",
    "\n",
    "# Attempt to print preprocessing steps, if available\n",
    "try:\n",
    "    preprocessing_1 = base_model_1.get_preprocessor()\n",
    "    print(\"Preprocessing steps for LightGBMXT_BAG_L1:\", preprocessing_1)\n",
    "except AttributeError:\n",
    "    print(\"No separate preprocessor for this model; preprocessing was likely done prior to stacking.\")\n",
    "\n",
    "# Print details about the second base model\n",
    "print(\"Base Model 2: LightGBM_BAG_L1\")\n",
    "print(\"Hyperparameters:\", base_model_2.params)\n",
    "print(\"Features used:\", base_model_2.features)\n",
    "\n",
    "# Attempt to print preprocessing steps, if available\n",
    "try:\n",
    "    preprocessing_2 = base_model_2.get_preprocessor()\n",
    "    print(\"Preprocessing steps for LightGBM_BAG_L1:\", preprocessing_2)\n",
    "except AttributeError:\n",
    "    print(\"No separate preprocessor for this model; preprocessing was likely done prior to stacking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce9f815-bdb2-4b6b-85fe-bcc115e6a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model 1: LightGBMXT_BAG_L1\n",
      "Hyperparameters: {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}\n",
      "Features used: ['cap-diameter', 'stem-height', 'stem-width', 'cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color', 'stem-root', 'stem-surface', 'stem-color', 'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color', 'habitat', 'season']\n",
      "No separate preprocessor for this model; preprocessing was likely done prior to stacking.\n",
      "Base Model 2: LightGBM_BAG_L1\n",
      "Hyperparameters: {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}\n",
      "Features used: ['cap-diameter', 'stem-height', 'stem-width', 'cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color', 'stem-root', 'stem-surface', 'stem-color', 'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color', 'habitat', 'season']\n",
      "No separate preprocessor for this model; preprocessing was likely done prior to stacking.\n"
     ]
    }
   ],
   "source": [
    "# Access base models from the _trainer.models dictionary\n",
    "base_model_1 = predictor._trainer.load_model('LightGBMXT_BAG_L1')\n",
    "base_model_2 = predictor._trainer.load_model('LightGBM_BAG_L1')\n",
    "\n",
    "# Print details about the first base model\n",
    "print(\"Base Model 1: LightGBMXT_BAG_L1\")\n",
    "print(\"Hyperparameters:\", base_model_1.params)\n",
    "print(\"Features used:\", base_model_1.features)\n",
    "\n",
    "# Attempt to print preprocessing steps, if available\n",
    "try:\n",
    "    preprocessing_1 = base_model_1.preprocess_transformations\n",
    "    print(\"Preprocessing steps for LightGBMXT_BAG_L1:\", preprocessing_1)\n",
    "except AttributeError:\n",
    "    print(\"No separate preprocessor for this model; preprocessing was likely done prior to stacking.\")\n",
    "\n",
    "# Print details about the second base model\n",
    "print(\"Base Model 2: LightGBM_BAG_L1\")\n",
    "print(\"Hyperparameters:\", base_model_2.params)\n",
    "print(\"Features used:\", base_model_2.features)\n",
    "\n",
    "# Attempt to print preprocessing steps, if available\n",
    "try:\n",
    "    preprocessing_2 = base_model_2.preprocess_transformations\n",
    "    print(\"Preprocessing steps for LightGBM_BAG_L1:\", preprocessing_2)\n",
    "except AttributeError:\n",
    "    print(\"No separate preprocessor for this model; preprocessing was likely done prior to stacking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33732a35-021c-45f2-8440-72b1612c43e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
